# 决策树之随机森林

[在 CART 分类回归树]((/AI/cart1.md))的基础之上，我们可以很容易的掌握随机森林算法，它们之间的区别在于，CART 决策树较容易过拟合，而随机森林可以在一定程度上解决该问题。

随机森林的主要思想是：使用随机性产生出一系列简单的决策树，并组合它们的预测结果为最终的结果，可谓三个臭皮匠赛过一个诸葛亮，下面我们就来具体了解一下。

## 产生随机森林的具体步骤

产生随机森林的步骤大致为三步

1. 准备样本
2. 产生决策树
3. 循环第 1 、2 步，直到产生足够的决策树，一般为上百个

在第 1 步，它是一个可放回抽样，即所产生的样本是允许重复的，这种抽样又被称为 Bootstrap，例如我们有以下 dummy 数据

| 胸口疼痛 | 血液循环正常 | 血管堵塞 | 体重 | 患心脏病 |
| -------- | ------------ | -------- | ---- | -------- |
| No       | No           | No       | 125  | No       |
| Yes      | Yes          | Yes      | 180  | Yes      |
| Yes      | Yes          | No       | 210  | No       |
| Yes      | No           | Yes      | 167  | Yes      |

在做完 Bootstrap 之后，可能的样本数据如下

| 胸口疼痛 | 血液循环正常 | 血管堵塞 | 体重 | 患心脏病 |
| -------- | ------------ | -------- | ---- | -------- |
| Yes      | Yes          | Yes      | 180  | Yes      |
| No       | No           | No       | 125  | No       |
| Yes      | No           | Yes      | 167  | Yes      |
| Yes      | No           | Yes      | 167  | Yes      |

可见，样本数据中，第 3 条和第 4 条样本是一样的，都对应的是原始数据中的第 4 条。

接下来，就是要使用上面的样本数据来产生决策树了，产生决策树的方法和 CART 基本一致，唯一的不同地方在于，节点的构建不是来自于全部的候选特征，而是先从中随机的选择 n 个特征，在这 n 个特征中找出一个作为最佳节点。

举个例子，假设 n = 2，且我们随机选择了「血液循环正常」和「血管堵塞」这两个特征来产生根节点，如下：

| 血液循环正常 | 血管堵塞 | 患心脏病 |
| ------------ | -------- | -------- |
| Yes          | Yes      | Yes      |
| No           | No       | No       |
| No           | Yes      | Yes      |
| No           | Yes      | Yes      |

我们将在上述两个特征中选择一个合适的特征作为根节点，假设在计算完 Gini 不纯度之后，「血液循环正常」这个特征胜出，那么我们的根节点便是「血液循环正常」，如下图所示

![rf_root](https://github.com/jieniu/articles/blob/master/docs/.vuepress/public/rf_root.png?raw=true)

接下来我们还需要构建根节点下面的节点，下一个节点将会在剩下的「胸口疼痛」、「血管堵塞」和「体重」三个特征中产生，但我们依然不会计算所有这 3 个特征的 Gini 不纯度，而是从中随机选择 2 个特征，取这 2 个特征中的 Gini 不纯度较低者作为节点。

例如我们随机选到了「胸口疼痛」和「体重」这两列，如下：

| 胸口疼痛 | 体重 | 患心脏病 |
| -------- | ---- | -------- |
| Yes      | 180  | Yes      |
| No       | 125  | No       |
| Yes      | 167  | Yes      |
| Yes      | 167  | Yes      |

假设此时「体重」的 Gini 不纯度更低，那么第 2 个节点便是「体重」，如下图：

![rf_node](https://github.com/jieniu/articles/blob/master/docs/.vuepress/public/rf_node.png?raw=true)

继续下去，我们便产生了一棵决策树。

随机森林是多棵决策树，在产生完一棵决策树后，接着会循环执行上述过程：Bootstrap 出训练样本，训练决策树，直到树的数量达到设置值——通常为几百棵树。

## 随机森林的预测

现在我们产生了几百棵树的随机森林，当我们要预测一条数据时，该怎么做呢？我们会聚合这些树的结果，选择预测结果最多的那个分类作为最终的预测结果。

例如我们现在有一条数据：

| 胸口疼痛 | 血液循环正常 | 血管堵塞 | 体重 | 患心脏病 |
| -------- | ------------ | -------- | ---- | -------- |
| Yes      | No           | No       | 168  |          |

该条数据被所有树预测的结果如下：

| 第几颗树 | 预测结果 |
| -------- | -------- |
| 1        | Yes      |
| 2        | Yes      |
| ...      | ...      |
| 100      | No       |

上述结果聚合后为：

| 预测结果 | 次数 |
| -------- | ---- |
| Yes      | 82   |
| No       | 18   |

取最多的那项为最终的预测结果，即 Yes——该病人被诊断为患有心脏病。

以上，随机森林的两个过程：**B**ootstrap 和 **Agg**regate 又被称为 **Bagging**。

## 总结

本文我们一起学习了随机森林的算法，和 CART 决策树比起来，它主要被用来解决过拟合问题，其主要的思想为 Bagging，即随机性有助于增强模型的泛化（Variance） 能力。



参考：

* [Random Forests](https://www.youtube.com/watch?v=6EXPYzbfLCE&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=39)



相关文章：

* [决策树算法之分类回归树 CART（Classification and Regression Trees）【1】](/AI/cart1.md)

* [决策树算法之分类回归树 CART（Classification and Regression Trees）【2】](/AI/cart2.md)



