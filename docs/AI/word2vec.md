# 读懂 Word2vec

Word2vec 论文由 Google 的研究团队发布于 2013 年，它的发布，很大程度上改变了 NLP 技术的发展，不仅如此，在使用神经网络来解决各个领域的问题时，谈必离不开 Embedding，而 Embedding 究竟是什么？了解 Word2vec 的同学都知道，它其实是 Word2vec 的另一个名字，或广义的 Word2vec，**是一种使用稠密向量来表示特征的表示学习方法**。

例如在搜索与推荐领域，Word2vec 的思想可以用来对 Item、Query 和 User 等关键特征编码，产生 Item、Query 和 User 的稠密向量，有了这些向量后，就可以进一步将它们用于召回和排序场景中。可以毫不夸张的说，它是智能化搜索和推荐系统的基础，或者说 Word2vec 的出现，推进了搜索与推荐领域智能化的发展。

而对于我来说，Word2vec 是让我产生顿悟的一项技术，大体原因是它的核心思想到技术实现的过度不是那么直接，比如：类似“它是怎么更新向量参数的”这样的问题。可能是因为看了一些在线课程，以及模型的训练代码后，之前遗留的一系列问题突然间全明白了，与此同时，更领悟到这项技术的重要性。

虽然网络上也有一些介绍该项技术的优秀文章，我还是想把它写下来，一方面是检验我对它的理解，另一方面，也希望能帮助像我一样、一开始也在这项技术的理解上“挣扎”的同学，能够快速理解它。

_预备知识：本文假设你已经了解 one-hot 编码、逻辑回归、Softmax、内积这些概念。_

## word2vec 的核心思想

在 NLP 自然语言处理领域，特征是由词（word）组成的，要让计算机处理这些 word，就需要把它们编码成数字，因为 word 是离散型数据，在过去，最直观的编码方式就是采用 one-hot 编码，而 one-hot 编码的一个很明显的问题就是任意两个向量之间是正交的，无法用它们的内积或余弦相似度来表达词与词之间的相关性，而这种语义相关性又是非常重要的。

既然 one-hot 这种稀疏的编码方式不合适，那么我们很自然的就会想到使用稠密向量来表示每一个词，这就好比我们玩的足球游戏中的雷达图，如下图所示：

![radar_encoding](https://github.com/jieniu/articles/blob/master/docs/.vuepress/public/radar_encoding.png?raw=true)

游戏中的雷达图用 6 个维度来表示世界上的全部球员，每个球员用这 6 个维度上的不同取值来表达能力的差异，在某些维度上数值相近的球员，会比较“相似”；例如射门次数和射门转化率都很高的球员，他们很可能都是优秀的前锋。

实际上，上述雷达图就是一种将球员编码成稠密向量的方式，即上文提到的 Embedding，但机器学习中的 Embedding 的维度往往多得多，且每个维度的解释性也没有雷达图这么直观。

以上是我们 Word2vec 的编码目标，接下来就需要确定该如何来编码了，这个编码方式的 idea 是这样的：

> You shall know a word by the company it keeps. (J. R. Firth 1957: 11) -- from cs224n

意思是，**那些经常出现在 word 周围的词，给予了这个 word 的含义**。而 skip-gram 和 CBOW（Continuous Bag Of Words 连续词袋模型） 就是两种围绕着这个核心思想来对 word 进行编码的算法。

## Skip-gram 模型

skip-gram 表示为根据中心词来预测其周围的词，而 CBOW 是反过来，它根据中心词周围的词来预测中心词。对于 “the man loves his son” 窗口，loves 是该窗口的中心词，而其他 4 个词，本文称它们为窗口词，在训练时，skip-gram 和 CBOW 方法分别表示如下：

![skipgram_cbow](https://github.com/jieniu/articles/blob/master/docs/.vuepress/public/skipgram_cbow.png?raw=true)

因为在实际运用中，skip-gram 更为常见，后文主要以该方法进行说明。上面左图可以用一个条件概率来表达：
$$
p(\text{the},\text{man},\text{his},\text{son} | \text{loves})
$$
假设每个词出现的概率都是独立的，则上式可分解为：
$$
p(\text{the} \mid \text{loves})\cdot p(\text{man}\mid \text{loves})\cdot p(\text{his}\mid \text{loves})\cdot p(\text{son} \mid \text{loves})
$$
我们希望上面的条件概率越大越好，于是，对于语料库中每一组中心词所产生出的窗口词的条件概率，将它们相乘，就是模型需要优化的指标，或 Loss，如下：
$$
\prod_{c=1}^{T} \prod_{j\le|m|,j\ne0} p(w_{c+j}|w_c)
$$
上式中，T 表示窗口数量，也是中心词的个数，$w_c$ 表示中心词，$j\le |m|,j\ne0$ 表示 $w_{c+j}$ 在窗口中取词，且不会取到中心词本身，于是 $w_{c+j}$ 就表示为窗口词。

对上式求 -log，将乘法变成加法，把优化上式的最大值问题变成了优化下式的最小值问题：
$$
\sum_{c=1}^{T} \sum_{j\le|m|,j\ne0} -\log(p(w_{c+j}|w_c))
$$
定义了 Loss 后，剩下的问题是：如何求 $p(w_{c+j}|w_c)$ 的概率？

回到核心思想——一个词的含义可以用频繁出现在它周围的词来表示，意味着 $w_c$ 和 $w_{c+j}$ 比较相似，同时 $w_{c}$ 和其他的词的相似度较低。假设每个词的向量已经被训练出来了，$w_c$ 的向量为 $v_c$，而 $w_{c+j}$ 的向量为 $u_{o}$，则 $w_c$ 和 $w_{c+j}$ 的相似度可以通过它们各自向量的内积来计算：$v_c\cdot u_o$，同理，我们也可以计算中心词和语料库中其他词的相似度。

有了中心词和所有词的相似度之后，我们就可以利用 softmax 来将相似度转换为一个概率，而给定中心词，计算其窗口词的条件概率就可以如下计算：
$$
p(w_{c+j}|w_c) = \frac{\exp(u_o\cdot v_c)}{\sum_{i=1}^{V} \exp(u_i\cdot v_c)}
$$
上式中，$V$ 表示语料库中的独立词数，至此，整个 Loss 就推导完了。可以看到，表达式还是非常简洁的，其中，m（窗口大小）是一个超参数，而向量 $v$ 和 $u$ 就是模型需要优化的参数了。具体怎么优化，我们通过下面的神经网络图来进一步说明：

![word2vec_nn](https://github.com/jieniu/articles/blob/master/docs/.vuepress/public/word2vec_nn.png?raw=true)

上图中，输入层是一个 v 维向量，v 对应语料库中的独立词的数量，每一维代表一个词，图中输入的是语料库中的第 2 个词（下标为 1），可以看到该词输入时，仅第 2 个神经元的值为 1，其他神经元的值都为 0，可见，在训练词向量之前，每个词依然还是采用 one-hot 进行编码的。

输入层和隐藏层之间是一个全连接层，假设隐藏层的维度为 h，那么输入层到隐藏层之间的参数是一个 $v\times h$  的矩阵，我们用二维数组 `V[v][h]` 来表示它，为了方便起见，这里假设 h = 3。

当参数矩阵 V 和 one-hot 编码的输入层结合后，会出现一个特殊的现象：只有值为 1 的神经元（中心词）和隐藏层之间连接的参数会被激活，即图中所画的 3 个参数 `V[1][0]`、`V[1][1]` 和 `V[1][2]`，且通过前向传播后，这几个参数会传导到隐藏层的各个神经元。而被激活的这几个参数，就是我们所说的用来表示中心词的稠密向量，向量的维度由隐藏层的维度决定。

隐藏层到输出层的结构，和输入层到隐藏层的结构是一样的，因为我们需要计算中心词和语料库中所有词的相似度，每个相似度占输出层的 1 个维度，则输出层的神经元也是 v 维，矩阵 U 的形状也为 $v \times h$。

如果输出层的每个神经元是中心词和其他词的相似度，对于其中每一个神经元，其和隐藏层连接的参数就是用来表示对应词的稠密向量。例如，假设输出层的第 3 个神经元对应一个窗口词和中心词的相似度，那么连接该神经元到隐藏层的参数 `U[2][0]`、`U[2][1]` 和 `U[2][2]` 就是这个窗口词的词向量，因为正向传播时，这个神经元的计算过程如下：
$$
V[1][0]\cdot U[2][0]+V[1][1]\cdot U[2][1]+V[1][2]\cdot U[2][2]
$$
这正好是 `V[1][:]` 和 `U[2][:]` 这两个向量的内积，将输出层接上一个 softmax，就可以求出 $p(w_{c+j}|w_c)$。最后，在目标层标记出哪些是窗口词，就可以通过交叉熵损失函数来反向调整参数，从而优化损失函数，当 Loss 逐渐收敛，留下来的参数 U 和 V，就是我们需要的稠密词向量。

但此时你会发现，对于同一个词，上述神经网络会产出 2 组向量，分别是 V 矩阵中的 v 个向量和 U 矩阵中的 v 个向量，那我们取哪组向量作为词向量呢？一般我们取中心词对应的向量，即在 skip-gram 中，取输入层到隐藏层间的向量；而在 CBOW 中，取隐藏层到输出层间的向量。

上述过程看上去很完美，却存在一个问题，就是每迭代一个中心词，都会计算中心词和语料库中所有词的相似度，以及反向传播时会计算所有词向量的梯度，这种计算开销无疑是非常巨大的，所以我们需要对模型做进一步的优化。

## 负采样

为了降低模型训练时的计算量，我们通常会采用负采样技术来优化模型，顾名思义，负采样就是仅采样少量的负样本进行训练，我们把窗口内的词作为中心词的正样本，而采样 K 倍于正样本的窗口外的词作为负样本（K 通常取 5），于是，输出层的神经元由全量语料库的 v 个减少到 (m-1)(K+1) 个，这极大的减少了训练时的计算量。

窗口词为正样本，采样的少量非窗口词作为负样本，所以这里变成了一个二分类问题，使用 sigmoid 函数就够了，不需要 softmax 多分类模型，则上文中的条件表达式 $p(w_{c+j}|w_c)$ 可以修改如下：
$$
p(D=1|w_c,w_{c+j}) \prod_{k=1}^{K} p(D=0|w_c,w_k)
$$
意为中心词和窗口词共现的概率接近 1，而中心词和采样的 K 个非窗口词共现的概率接近 0，使用 Maximum Likelihood 来最大化该概率。同样，用 -log 将连乘的最大值转化为求和的最小值，如下：
$$
\begin{aligned}
-\log(p(w_{c+j}|w_c)) &= -\log(p(D=1|w_c,w_{c+j})) - \sum_{k=1}^{K} \log(p(D=0|w_c,w_k))\\
&= -\log(\sigma(v_c\cdot u_o)) - \sum_{k=1}^{K} \log(1-\sigma(v_c\cdot u_k)) \\
&= -\log(\sigma(v_c\cdot u_o)) - \sum_{k=1}^{K} \log(\sigma(-v_c\cdot u_k))
\end{aligned}
$$
上面这个式子经常会出现在各大论文中，看到这里的同学可以留意下。用这个式子更新了 Loss 后，对应的我们把网络结构也修改如下：

![word2vec_negsample](https://github.com/jieniu/articles/blob/master/docs/.vuepress/public/word2vec_negsample.png?raw=true)

主要变化的地方在于输出层由原先的 v 个神经元变为 (m-1)(K+1) 个，其中 m-1 个正样本，K(m-1) 个负样本，以及将 Softmax 层换成了 Sigmoid 计算。

需要注意的是，这里的隐藏层到输出层的参数 `U[(m-1)(K+1)][h]` 只是完整矩阵 U 的一小部分，完整 U 的矩阵大小依然为 $v \times h$ ，当前训练的子矩阵是由正负样本的参数构成的。

## 小结

以上，是整个 Word2vec 的原理及优化训练的全部内容。

Word2vec 技术在搜索推荐领域有着进一步的延伸，例如使用用户的行为序列作为 Word2vec 中的 word 序列，训练出表示 item 的稠密向量，被称为 Item2vec；以及根据用户、行为关系图，随机游走生成的序列，再利用这些序列来编码的技术 GraphEmbedding 等。

在 NLP 领域，一个 word 只产生一个 Embedding 是满足不了真实应用场景的，现实情况中很多 word 都有多个意思，需要用多个 Embedding 来表示，于是催生出了 ELMo、BERT 等新一代 Word2vec 技术，对这些知识感兴趣的同学可以继续研究。



参考：

* 动手深度学习-词嵌入(http://tinyurl.com/y8g37x2s)
* 《深度学习推荐系统》- Embedding 在推荐系统中的应用
* cs224n-lecture1(http://tinyurl.com/yd7vmkap)
